import feedparser
import requests
import json
import os
import re
from bs4 import BeautifulSoup
import time
from datetime import datetime, timedelta
from time import mktime

# --- è¨­å®šã‚¨ãƒªã‚¢ ---
DISCORD_WEBHOOK_URL = os.environ.get("DISCORD_WEBHOOK_URL")
DATA_FILE = "data.json"

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
}

# ã‚¹ãƒŠã‚¤ãƒ‘ãƒ¼ç”¨ï¼šæ¿ƒã„æƒ…å ±æºãƒªã‚¹ãƒˆ
RSS_URLS = [
    # ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ï¼ˆç¥­ã‚Šæ¤œçŸ¥ç”¨ï¼‰
    "https://b.hatena.ne.jp/entrylist/it.rss",
    "https://b.hatena.ne.jp/entrylist/life.rss",
    # Appleãƒ»ã‚¬ã‚¸ã‚§ãƒƒãƒˆ
    "https://gori.me/feed",
    "https://iphone-mania.jp/feed/",
    # ã‚²ãƒ¼ãƒ ç„¡æ–™ãƒ»PCç‰¹ä¾¡
    "https://automaton-media.com/feed/",
    "https://b2b.hack.ne.jp/feed",
]

# ã‚«ãƒ†ã‚´ãƒªå®šç¾©
CATEGORIES = [
    {
        "name": "ğŸš¨ ç·Šæ€¥ï¼šä¾¡æ ¼è¨­å®šãƒŸã‚¹ãƒ»ãƒã‚°ç–‘æƒ‘",
        "keywords": ["ä¾¡æ ¼å´©å£Š", "è¨­å®šãƒŸã‚¹", "è¡¨è¨˜ãƒŸã‚¹", "ä¾¡æ ¼ãƒŸã‚¹", "0å††è¨­å®š", "æ¡é–“é•ã„", "90%OFF", "99%OFF"],
        "color": 0xFF0000,
        "priority": True 
    },
    {
        "name": "ğŸ Appleæ•´å‚™æ¸ˆãƒ»å¾©æ´»",
        "keywords": ["æ•´å‚™æ¸ˆ", "MacBook", "iPad", "èªå®šæ•´å‚™", "å†å…¥è·", "åœ¨åº«å¾©æ´»"],
        "color": 0xFFFFFF,
        "priority": False
    },
    {
        "name": "ğŸ® ã‚²ãƒ¼ãƒ ãƒ»ã‚½ãƒ•ãƒˆ 100%OFF",
        "keywords": ["ç„¡æ–™é…å¸ƒ", "æœŸé–“é™å®šç„¡æ–™", "100%OFF", "ã‚¿ãƒ€", "ãƒ—ãƒ¬ã‚¼ãƒ³ãƒˆ", "é…å¸ƒé–‹å§‹"],
        "color": 0x00FF00,
        "priority": False
    },
    {
        "name": "ğŸ’° ãã®ä»– æ¿€ã‚¢ãƒ„æ¡ˆä»¶",
        "keywords": ["ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒãƒƒã‚¯", "å…¨å“¡", "ç¥æ¡ˆä»¶", "ãƒãƒ©ãƒã‚­", "åˆ©ç›Š"],
        "color": 0xFFA500,
        "priority": False
    }
]

def load_sent_data():
    if os.path.exists(DATA_FILE):
        try:
            with open(DATA_FILE, 'r', encoding='utf-8') as f: return json.load(f)
        except: return []
    return []

def save_sent_data(data):
    with open(DATA_FILE, 'w', encoding='utf-8') as f:
        json.dump(data[-300:], f, ensure_ascii=False, indent=2)

def is_within_24h(entry):
    if not hasattr(entry, 'published_parsed'): return True
    published_time = datetime.fromtimestamp(mktime(entry.published_parsed))
    return (datetime.now() - published_time) < timedelta(hours=24)

def extract_code_simple(text):
    pattern = r'(?:ã‚³ãƒ¼ãƒ‰|ã‚¯ãƒ¼ãƒãƒ³)[:ï¼š]\s*([a-zA-Z0-9\-_]{4,20})'
    match = re.search(pattern, text)
    if match:
        code = match.group(1)
        if not re.search(r'(202[0-9]|http)', code):
            return code
    return None

def send_discord(category, title, link, code, source_name):
    content_text = ""
    if category["priority"]:
        content_text = "@everyone ğŸš¨ **ç·Šæ€¥é€Ÿå ±ï¼ä¾¡æ ¼ãƒŸã‚¹ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ï¼**"

    description = f"**{title}**\n\n"
    if code: description += f"ğŸ« ã‚¯ãƒ¼ãƒãƒ³ã‚³ãƒ¼ãƒ‰:\n```{code}```\n"
    else: description += "ğŸ”— **ã‚³ãƒ¼ãƒ‰ä¸è¦ã¾ãŸã¯ãƒªãƒ³ã‚¯å…ˆã§ç¢ºèª**\n"
    description += f"\n[ğŸ‘‰ å•†å“ãƒšãƒ¼ã‚¸ãƒ»è©³ç´°ã‚’è¦‹ã‚‹]({link})"

    embed = {
        "title": category["name"],
        "description": description,
        "url": link,
        "color": category["color"],
        "footer": {"text": f"Source: {source_name}"}
    }

    try:
        requests.post(DISCORD_WEBHOOK_URL, json={"username": "Sniper Bot", "content": content_text, "embeds": [embed]})
        time.sleep(2)
    except Exception as e:
        print(f"Error: {e}")

def main():
    if not DISCORD_WEBHOOK_URL: return
    sent_urls = load_sent_data()
    new_sent_urls = sent_urls.copy()
    print("Sniping targets...")

    for rss_url in RSS_URLS:
        try:
            resp = requests.get(rss_url, headers=HEADERS, timeout=10)
            feed = feedparser.parse(resp.content)
            source_name = feed.feed.title if 'title' in feed.feed else "Web"

            for entry in feed.entries[:5]:
                link = entry.link
                title = entry.title
                if link in sent_urls: continue
                if not is_within_24h(entry): continue

                matched_category = None
                for cat in CATEGORIES:
                    text_to_check = title
                    if 'summary' in entry: text_to_check += entry.summary
                    if any(k in text_to_check for k in cat["keywords"]):
                        matched_category = cat
                        break 
                
                if matched_category:
                    print(f"ğŸ¯ HIT [{matched_category['name']}]: {title}")
                    description = entry.summary if 'summary' in entry else ""
                    code = extract_code_simple(BeautifulSoup(description, "html.parser").get_text())
                    send_discord(matched_category, title, link, code, source_name)
                    new_sent_urls.append(link)

        except Exception as e:
            print(f"Error checking {rss_url}: {e}")
            continue

    save_sent_data(new_sent_urls)
    print("Mission Complete.")

if __name__ == "__main__":
    main()
